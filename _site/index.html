<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Qing  Wu</title>
    <meta name="author" content="Qing  Wu" />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/friend/">Friends</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Qing</span>  Wu
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/wq.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="wq.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<div class="social">
              <div class="contact-note">
                Feel free to find me in any way you like :)

              </div>
              <div class="contact-icons">
              <a href="mailto:%77%75%71%69%6E%67@%73%68%61%6E%67%68%61%69%74%65%68.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0001-9320-0332" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=A1E80HUAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/iwuqing" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            

              </div>
            </div>
          </div>

          <div class="clearfix">
            <p>I am Qing Wu (吴晴 in Chinese), a thrid-year Ph.D. student in SMILE<img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20"> LAB at <a href="https://www.shanghaitech.edu.cn/eng/" target="_blank" rel="noopener noreferrer">ShanghaiTech University</a>, advised by Prof. <a href="https://sist.shanghaitech.edu.cn/sist_en/2020/0814/c7582a54827/page.htm" target="_blank" rel="noopener noreferrer">Yuyao Zhang</a>. Before that, I received my B.E. degree at <a href="https://en.cug.edu.cn/" target="_blank" rel="noopener noreferrer">China University of Geosciences, Wuhan</a> in 2020.</p>

<p>I am interested in a variety of topics related to <strong>medical image processing</strong>. Currently, my research is focused on: <br></p>
<ul>
  <li>Neural Radiance Field<br>
</li>
  <li>Sparse-View CT Reconstruction<br>
</li>
  <li>CT Metal Artifacts Reduction<br>
</li>
  <li>CT Motion Correction<br>
</li>
  <li>MRI Super-Resolution<br>
</li>
</ul>

<!-- See more in the [detailed CV](http://lattes.cnpq.br/4761632587625158). -->

          </div>


          <!-- News -->          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Jan 24, 2023</th>
                  <td>
                    Three papers were accepted by the 20th IEEE International Symposium on Biomedical Imaging (IEEE ISBI 2023) <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Dec 17, 2022</th>
                  <td>
                    One paper was accepted by IEEE Transactions on Circuits and Systems I: Regular Papers (IEEE TCAS-I) <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Nov 26, 2022</th>
                  <td>
                    One paper was accepted by IEEE Journal of Biomedical and Health Informatics (IEEE J-BHI) <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20">
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>selected publications</h2>
            <p>*denotes co-first authors.</p>
            <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3 " style="text-align:center">
<abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)">
      <div style="color:var(--global-bg-color);">IEEE ISBI 2023</div></abbr><br><p></p>
<img id="isbi_coverpng" class="preview z-depth-1 rounded myImg" src="/assets/pub_img/isbi_cover.png"><div id="isbi_coverpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('isbi_coverpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="isbi_coverpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("isbi_coverpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('isbi_coverpng');
      var modalImg = document.getElementById("isbi_coverpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function() {
        modal.style.display = "none";
      }
      modal.onclick = function() {
        modal.style.display = "none";
      }
    </script>
</div>
  <!-- Entry bib key -->
  <div id="https://doi.org/10.48550/arxiv.2210.12731" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating Neural Field</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Xin Li, Hongjiang Wei, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE International Symposium on Biomedical Imaging 2023</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/abs/2210.12731" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Inspired by the recent Neural Radiance Field (NeRF) work, Implicit Neural Representation (INR) has widely received attention in Sparse-View Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep learning framework. INR-based SVCT methods represent the desired CT image as a continuous function of spatial coordinates and train a Multi-Layer Perceptron (MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting from the continuous image function provided by INR, high-quality CT image can be reconstructed. However, existing INR-based SVCT methods suppose by default that there is no relative motion during CT image acquisition. Therefore, such methods suffer from severe performance drops for real SVCT imaging with even minor subject motion. In this work, we propose a self-calibrating neural field to recover the artifacts-free image from rigid motion-corrupted SV sinogram without using any external training data. Specifically, we introduce a transform matrix for each projection pose in the sinogram respectively, to present the the inaccurate projection poses caused by subject rigid motion. Then we optimize these transformation matrices and the CT image jointly for achieving rigid motion corrected CT image reconstruction. We conduct numerical experiments on a public CT image dataset. The results indicate our model significantly outperforms two representative INR-based methods for SVCT reconstruction tasks with four different levels of rigid motion.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 " style="text-align:center">
<abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)">
      <div style="color:var(--global-bg-color);">IEEE J-BHI 2023</div></abbr><br><p></p>
<img id="arssr_coverjpg" class="preview z-depth-1 rounded myImg" src="/assets/pub_img/arssr_cover.jpg"><div id="arssr_coverjpg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('arssr_coverjpg-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="arssr_coverjpg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("arssr_coverjpg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('arssr_coverjpg');
      var modalImg = document.getElementById("arssr_coverjpg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function() {
        modal.style.display = "none";
      }
      modal.onclick = function() {
        modal.style.display = "none";
      }
    </script>
</div>
  <!-- Entry bib key -->
  <div id="9954892" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">An Arbitrary Scale Super-Resolution Approach for 3D MR Images via Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Yuwei Li, Yawen Sun, Yan Zhou, Hongjiang Wei, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE Journal of Biomedical and Health Informatics</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/9954892" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      <a href="https://github.com/iwuqing/ArSSR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>High Resolution (HR) medical images provide rich anatomical structure details to facilitate early and accurate diagnosis. In magnetic resonance imaging (MRI), restricted by hardware capacity, scan time, and patient cooperation ability, isotropic 3-dimensional (3D) HR image acquisition typically requests long scan time and, results in small spatial coverage and low signal-to-noise ratio (SNR). Recent studies showed that, with deep convolutional neural networks, isotropic HR MR images could be recovered from low-resolution (LR) input via single image super-resolution (SISR) algorithms. However, most existing SISR methods tend to approach scale-specific projection between LR and HR images, thus these methods can only deal with fixed up-sampling rates. In this paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for recovering 3D HR MR images. In the ArSSR model, the LR image and the HR image are represented using the same implicit neural voxel function with different sampling rates. Due to the continuity of the learned implicit function, a single ArSSR model is able to achieve arbitrary and infinite up-sampling rate reconstructions of HR images from any input LR image. Then the SR task is converted to approach the implicit voxel function via deep neural networks from a set of paired HR and LR training examples. The ArSSR model consists of an encoder network and a decoder network. Specifically, the convolutional encoder network is to extract feature maps from the LR input images and the fully-connected decoder network is to approximate the implicit voxel function. Experimental results on three datasets show that the ArSSR model can achieve state-of-the-art SR performance for 3D HR MR image reconstruction while using a single trained model to achieve arbitrary up-sampling scales.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 " style="text-align:center">
<abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)">
      <div style="color:var(--global-bg-color);">IEEE TCAS-I 2023</div></abbr><br><p></p>
<img id="tcas_coverpng" class="preview z-depth-1 rounded myImg" src="/assets/pub_img/tcas_cover.png"><div id="tcas_coverpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('tcas_coverpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="tcas_coverpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("tcas_coverpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('tcas_coverpng');
      var modalImg = document.getElementById("tcas_coverpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function() {
        modal.style.display = "none";
      }
      modal.onclick = function() {
        modal.style.display = "none";
      }
    </script>
</div>
  <!-- Entry bib key -->
  <div id="10004486" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">An Energy-Efficient Accelerator for Medical Image Reconstruction From Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    Chaolin Rao*, <em>Qing Wu*</em>, Pingqiang Zhou, Jingyi Yu, Yuyao Zhang, and Xin Lou</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE Transactions on Circuits and Systems I: Regular Papers</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/10004486" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This work presents an energy-efficient accelerator for medical image reconstruction from implicit neural representation (INR). The accelerator implements an INR-based algorithm to deliver high-quality medical image reconstruction with arbitrary resolution from a compact implicit format. In particular, we propose a dedicated hardware architecture based on an optimized computation flow for the INR-based reconstruction algorithm, which co-designs data reuse and computation load. The proposed architecture takes in the coordinate of the intersection of three scans and outputs all the voxel intensities, minimizing the data movement between on-chip and off-chip. To validate the proposed accelerator, we build a proof-of-concept prototype demonstration system using field programmable gate array (FPGA). We also map our design to 40nm CMOS technology to measure the performance of the proposed accelerator. The implementation results show that, running at 400MHz, the proposed accelerator is capable of processing medical images with  256\times 256  resolution in real-time at 26.3 frames per second (FPS), with a power consumption of only 795 mW. Comparison results show that the performance, as well as the energy efficiency of the proposed accelerator, outperforms the central processing unit (CPU)-based and graphic processing unit (GPU)-based implementations.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 " style="text-align:center">
<abbr class="badge" style="background: var(--global-theme-color);border-style: solid;border-width: 2px;border-color:var(--global-theme-color)">
      <div style="color:var(--global-bg-color);">MICCAI 2021</div></abbr><br><p></p>
<img id="irem_coverpng" class="preview z-depth-1 rounded myImg" src="/assets/pub_img/irem_cover.png"><div id="irem_coverpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('irem_coverpng-modal').style.display='none'">×</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="irem_coverpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("irem_coverpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('irem_coverpng');
      var modalImg = document.getElementById("irem_coverpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function() {
        modal.style.display = "none";
      }
      modal.onclick = function() {
        modal.style.display = "none";
      }
    </script>
</div>
  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-87231-1_7" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>Medical Image Computing and Computer Assisted Intervention 2021</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-87231-1_7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate, and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observation using a fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high-frequency image features, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.</p>
    </div>
  </div>
</div>
</li>
</ol>
          </div>


          
          <!-- <div class="social">
            <div class="contact-icons">
            <a href="mailto:%77%75%71%69%6E%67@%73%68%61%6E%67%68%61%69%74%65%68.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0001-9320-0332" title="ORCID"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=A1E80HUAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/iwuqing" title="GitHub"><i class="fab fa-github"></i></a>
            

            </div>

            <div class="contact-note">
              Feel free to find me in any way you like :)

            </div>
          </div> -->
          <!-- Social -->
          <!-- <div class="social">
            <div class="contact-icons">
            <a href="mailto:%77%75%71%69%6E%67@%73%68%61%6E%67%68%61%69%74%65%68.%65%64%75.%63%6E" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0001-9320-0332" title="ORCID"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=A1E80HUAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/iwuqing" title="GitHub"><i class="fab fa-github"></i></a>
            

            </div>

            <div class="contact-note">
              Feel free to find me in any way you like :)

            </div>

          </div> -->
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Qing  Wu. Last updated: March 18, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
