<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Qing  Wu</title>
    <meta name="author" content="Qing  Wu" />
    <meta name="description" content="publications by categories in reversed chronological order." />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Qing </span>Wu</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/friend/">Friends</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description">publications by categories in reversed chronological order.</p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">IEEE ISBI 2023</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/isbi_cover.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="https://doi.org/10.48550/arxiv.2210.12731" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">Joint Rigid Motion Correction and Sparse-View CT via Self-Calibrating Neural Field</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Xin Li, Hongjiang Wei, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE International Symposium on Biomedical Imaging 2023</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/abs/2210.12731" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Inspired by the recent Neural Radiance Field (NeRF) work, Implicit Neural Representation (INR) has widely received attention in Sparse-View Computed Tomography (SVCT) reconstruction tasks as a self-supervised deep learning framework. INR-based SVCT methods represent the desired CT image as a continuous function of spatial coordinates and train a Multi-Layer Perceptron (MLP) to learn the function by minimizing loss on the SV sinogram. Benefiting from the continuous image function provided by INR, high-quality CT image can be reconstructed. However, existing INR-based SVCT methods suppose by default that there is no relative motion during CT image acquisition. Therefore, such methods suffer from severe performance drops for real SVCT imaging with even minor subject motion. In this work, we propose a self-calibrating neural field to recover the artifacts-free image from rigid motion-corrupted SV sinogram without using any external training data. Specifically, we introduce a transform matrix for each projection pose in the sinogram respectively, to present the the inaccurate projection poses caused by subject rigid motion. Then we optimize these transformation matrices and the CT image jointly for achieving rigid motion corrected CT image reconstruction. We conduct numerical experiments on a public CT image dataset. The results indicate our model significantly outperforms two representative INR-based methods for SVCT reconstruction tasks with four different levels of rigid motion.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">IEEE J-BHI 2023</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/arssr_cover.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="9954892" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">An Arbitrary Scale Super-Resolution Approach for 3D MR Images via Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Yuwei Li, Yawen Sun, Yan Zhou, Hongjiang Wei, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE Journal of Biomedical and Health Informatics</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/9954892" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      <a href="https://github.com/iwuqing/ArSSR" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>High Resolution (HR) medical images provide rich anatomical structure details to facilitate early and accurate diagnosis. In magnetic resonance imaging (MRI), restricted by hardware capacity, scan time, and patient cooperation ability, isotropic 3-dimensional (3D) HR image acquisition typically requests long scan time and, results in small spatial coverage and low signal-to-noise ratio (SNR). Recent studies showed that, with deep convolutional neural networks, isotropic HR MR images could be recovered from low-resolution (LR) input via single image super-resolution (SISR) algorithms. However, most existing SISR methods tend to approach scale-specific projection between LR and HR images, thus these methods can only deal with fixed up-sampling rates. In this paper, we propose ArSSR, an Arbitrary Scale Super-Resolution approach for recovering 3D HR MR images. In the ArSSR model, the LR image and the HR image are represented using the same implicit neural voxel function with different sampling rates. Due to the continuity of the learned implicit function, a single ArSSR model is able to achieve arbitrary and infinite up-sampling rate reconstructions of HR images from any input LR image. Then the SR task is converted to approach the implicit voxel function via deep neural networks from a set of paired HR and LR training examples. The ArSSR model consists of an encoder network and a decoder network. Specifically, the convolutional encoder network is to extract feature maps from the LR input images and the fully-connected decoder network is to approximate the implicit voxel function. Experimental results on three datasets show that the ArSSR model can achieve state-of-the-art SR performance for 3D HR MR image reconstruction while using a single trained model to achieve arbitrary up-sampling scales.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">IEEE TCAS-I 2023</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/tcas_cover.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="10004486" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">An Energy-Efficient Accelerator for Medical Image Reconstruction From Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    Chaolin Rao*, <em>Qing Wu*</em>, Pingqiang Zhou, Jingyi Yu, Yuyao Zhang, and Xin Lou</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>IEEE Transactions on Circuits and Systems I: Regular Papers</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/abstract/document/10004486" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This work presents an energy-efficient accelerator for medical image reconstruction from implicit neural representation (INR). The accelerator implements an INR-based algorithm to deliver high-quality medical image reconstruction with arbitrary resolution from a compact implicit format. In particular, we propose a dedicated hardware architecture based on an optimized computation flow for the INR-based reconstruction algorithm, which co-designs data reuse and computation load. The proposed architecture takes in the coordinate of the intersection of three scans and outputs all the voxel intensities, minimizing the data movement between on-chip and off-chip. To validate the proposed accelerator, we build a proof-of-concept prototype demonstration system using field programmable gate array (FPGA). We also map our design to 40nm CMOS technology to measure the performance of the proposed accelerator. The implementation results show that, running at 400MHz, the proposed accelerator is capable of processing medical images with  256\times 256  resolution in real-time at 26.3 frames per second (FPS), with a power consumption of only 795 mW. Comparison results show that the performance, as well as the energy efficiency of the proposed accelerator, outperforms the central processing unit (CPU)-based and graphic processing unit (GPU)-based implementations.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">MICCAI 2022</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/n2sr_cover.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="10.1007/978-3-031-16446-0_32" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image</div>
    <!-- Author -->
    <div class="author">
    

    Xuanyu Tian, <em>Qing Wu</em>, Hongjiang Wei, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>Medical Image Computing and Computer Assisted Intervention 2021</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-16446-0_32" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Fluorescence microscopy is a key driver to promote discoveries of biomedical research. However, with the limitation of microscope hardware and characteristics of the observed samples, the fluorescence microscopy images are susceptible to noise. Recently, a few self-supervised deep learning (DL) denoising methods have been proposed. However, the training efficiency and denoising performance of existing methods are relatively low in real scene noise removal. To address this issue, this paper proposed self-supervised image denoising method Noise2SR (N2SR) to train a simple and effective image denoising model based on single noisy observation. Our Noise2SR denoising model is designed for training with paired noisy images of different dimensions. Benefiting from this training strategy, Noise2SR is more efficiently self-supervised and able to restore more image details from a single noisy observation. Experimental results of simulated noise and real microscopy noise removal show that Noise2SR outperforms two blind-spot based self-supervised deep learning image denoising methods. We envision that Noise2SR has the potential to improve more other kind of scientific imaging quality.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">MCCAI PIPPI 2022</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/MICCAI_workshop_PIPPI.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="chen2022continuous" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">Continuous longitudinal fetus brain atlas construction via implicit neural representation</div>
    <!-- Author -->
    <div class="author">
    

    Lixuan Chen, Jiangjie Wu, <em>Qing Wu</em>, Hongjiang Wei, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    <div style="color:red"><b>Best Paper Honorable Mention</b></div>
    
    
    <div class="periodical">
      <i>MICCAI workshop PIPPI 2022</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-17117-8_4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Longitudinal fetal brain atlas is a powerful tool for understanding and characterizing the complex process of fetus brain development. Existing fetus brain atlases are typically constructed by averaged brain images on discrete time points independently over time. Due to the differences in onto-genetic trends among samples at different time points, the resulting atlases suffer from temporal inconsistency, which may lead to estimating error of the brain developmental characteristic parameters along the timeline. To this end, we proposed a multi-stage deep-learning framework to tackle the time inconsistency issue as a 4D (3D brain volume + 1D age) image data denoising task. Using implicit neural representation, we construct a continuous and noise-free longitudinal fetus brain atlas as a function of the 4D spatial-temporal coordinate. Experimental results on two public fetal brain atlases (CRL and FBA-Chinese atlases) show that the proposed method can significantly improve the atlas temporal consistency while maintaining good fetus brain structure representation. In addition, the continuous longitudinal fetus brain atlases can also be extensively applied to generate finer 4D atlases in both spatial and temporal resolution.</p>
    </div>
  </div>
</div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">MICCAI 2021</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/irem_cover.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="10.1007/978-3-030-87231-1_7" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">IREM: High-Resolution Magnetic Resonance Image Reconstruction via Implicit Neural Representation</div>
    <!-- Author -->
    <div class="author">
    

    <em>Qing Wu</em>, Yuwei Li, Lan Xu, Ruiming Feng, Hongjiang Wei, Qing Yang, Boliang Yu, Xiaozhao Liu, Jingyi Yu, and Yuyao Zhang</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>Medical Image Computing and Computer Assisted Intervention 2021</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-87231-1_7" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>For collecting high-quality high-resolution (HR) MR image, we propose a novel image reconstruction network named IREM, which is trained on multiple low-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR image reconstruction. In this work, we suppose the desired HR image as an implicit continuous function of the 3D image spatial coordinate, and the thick-slice LR images as several sparse discrete samplings of this function. Then the super-resolution (SR) task is to learn the continuous volumetric function from a limited observation using a fully-connected neural network combined with Fourier feature positional encoding. By simply minimizing the error between the network prediction and the acquired LR image intensity across each imaging plane, IREM is trained to represent a continuous model of the observed tissue anatomy. Experimental results indicate that IREM succeeds in representing high-frequency image features, and in real scene data collection, IREM reduces scan time and achieves high-quality high-resolution MR imaging in terms of SNR and local image detail.</p>
    </div>
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr" style="text-align:center">
<abbr class="badge">NeuroImage 2021</abbr><p></p>
    <div class="teaser">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pub_img/fba_cover.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
</div>
  </div>
  <!-- Entry bib key -->
  <div id="wu2021age" class="col-sm-8">
  
    <!-- Title -->
    <div class="title">Age-specific structural fetal brain atlases construction and cortical development quantification for Chinese population</div>
    <!-- Author -->
    <div class="author">
    

    Jiangjie Wu, Taotao Sun, Boliang Yu, Zhenghao Li, <em>Qing Wu</em>, Yutong Wang, Zhaoxia Qian, Yuyao Zhang, Ling Jiang, and Hongjiang Wei</div>

    <!-- Journal/Book title and date -->
    
    
    
    <div class="periodical">
      <i>NeuroImage</i>
    </div>
  
    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.sciencedirect.com/science/article/pii/S105381192100687X" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In magnetic resonance imaging (MRI) studies of fetal brain development, structural brain atlases usually serve as essential references for the fetal population. Individual images are usually normalized into a common or standard space for analysis. However, the existing fetal brain atlases are mostly based on MR images obtained from Caucasian populations and thus are not ideal for the characterization of the fetal Chinese population due to neuroanatomical differences related to genetic factors. In this paper, we use an unbiased template construction algorithm to create a set of age-specific Chinese fetal atlases between 21-35 weeks of gestation from 115 normal fetal brains. Based on the 4D spatiotemporal atlas, the morphological development patterns, e.g., cortical thickness, cortical surface area, sulcal and gyral patterns, were quantified. The fetal brain abnormalities were detected when referencing the age-specific template. Additionally, a direct comparison of the Chinese fetal atlases and Caucasian fetal atlases reveals dramatic anatomical differences, mainly in the medial frontal and temporal regions. After applying the Chinese and Caucasian fetal atlases separately to an independent Chinese fetal brain dataset, we find that the Chinese fetal atlases result in significantly higher accuracy than the Caucasian fetal atlases in guiding brain tissue segmentation. These results suggest that the Chinese fetal brain atlases are necessary for quantitative analysis of the typical and atypical development of the Chinese fetal population in the future. The atlases with their parcellations are now publicly available at https://github.com/DeepBMI/FBA-Chinese.</p>
    </div>
  </div>
</div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Qing  Wu. Last updated: March 17, 2023.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>
